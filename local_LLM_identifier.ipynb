{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial setup\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import pyodbc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"pandas only supports SQLAlchemy connectable\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score, accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up local configuration for Llama 3\n",
    "client = OpenAI(\n",
    "    base_url = \"http://localhost:1234/v1\",\n",
    "    api_key = 'lm-studio'\n",
    ")\n",
    "deployment_name = 'bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test LLM\n",
    "start_phrase = 'Tagline for an ice cream shop'\n",
    "response = client.chat.completions.create(model=deployment_name, messages=[{\"role\":\"user\", \"content\":start_phrase}], max_tokens=10)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1 of the prompt for the LLM (entered in as the system context)\n",
    "system_context = \"\"\"This prompt consists of two sections: \"Instructions\" and \"Input,\" which contains a clinical note for you to parse.\n",
    "\n",
    "Section 1: Instructions\n",
    "\n",
    "Based on the past medical history data of the clinical note given in the Input Section, estimate the value and the certainty degree (CD: 0.00 to 1.00) for the following immunosuppression attributes with allowed values in the bracket:\n",
    "\n",
    "solid organ transplant: [yes, no]\n",
    "stem cell transplant: [yes, no]\n",
    "HIV: [yes, no]\n",
    "acute leukemia: [yes, no]\n",
    "lymphoma: [yes, no]\n",
    "multiple myeloma: [yes, no]\n",
    "immunoglobulin deficiency: [yes, no]\n",
    "\n",
    "Please follow the requirements below:\n",
    "\n",
    "1. For each attribute, create 3 key-value pairs:\n",
    "<attribute_estimation>: <the estimated value based on reading the note>\n",
    "<attribute_CD>: <the certainty degree of your estimation: [0.00, 1.00]>\n",
    "<attribute_evidence>: <the supporting evidence for your estimation>\n",
    "\n",
    "2. Please make sure to make valid inference for attribute estimate based on evidence. If there is no available evidence provided to make estimation, please answer the value as \"unknown\".\n",
    "\n",
    "3. Please make sure to output the whole set of answers together as a single JSON file, and don't output anything beyond the required JSON file.\"\"\"\n",
    "\n",
    "# Purpose: get the LLM response for a given prompt and the system_context defined above\n",
    "# Params:\n",
    "#   prompt: string, prompt to LLM (as user, beyond provided system_context)\n",
    "# Produces:\n",
    "#   response.choices[0].message.content: string, the LLM's response\n",
    "#   response.usage.total_tokens: number, number of total tokens\n",
    "#   response.usage.prompt_tokens: number, number of tokens in the prompt\n",
    "#   response.usage.completion_tokens: number, number of tokens in the response\n",
    "def get_completion(prompt):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_context\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Parameters to use\n",
    "    response = client.chat.completions.create(\n",
    "        model = deployment_name,\n",
    "        messages = messages,\n",
    "        temperature = 0,\n",
    "        max_tokens = 4000,\n",
    "        top_p = 1,\n",
    "        frequency_penalty = 0,\n",
    "        presence_penalty = 0\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content, response.usage.total_tokens, response.usage.prompt_tokens, response.usage.completion_tokens\n",
    "\n",
    "# Purpose: use get_completion() from above to process one row of the notes dataframe containing one note\n",
    "# Params:\n",
    "#   row: row of a dataframe, has study ID, note ID, note text\n",
    "# Produces:\n",
    "#   result, dictionary with LLM completion status, total tokens, prompt tokens, completion tokens, study ID, note ID, json-translated response\n",
    "def process_row(row):\n",
    "    success = False\n",
    "    num_tries = 0\n",
    "    \n",
    "    # Don't try more than 3 times if something is going wrong\n",
    "    while num_tries < 3 and success == False:\n",
    "        prompt = \"Section 2: Input\\n\\n\" + row['text']\n",
    "        try:\n",
    "            # Get the LLM response and reformat as needed\n",
    "            response, total_tokens, prompt_tokens, completion_tokens = get_completion(prompt)\n",
    "            success = True\n",
    "            # Store LLM output\n",
    "            result = {\"Status\": 1, \"Total Tokens\": str(total_tokens), \"Prompt Tokens\": str(prompt_tokens), \"Completion Tokens\": str(completion_tokens), \"SCRIPT_study_id\": row['SCRIPT_study_id'], \"note_id\": row['note_id'], \"response\": str(response)}\n",
    "            print (\"Current id: \" + str(row['note_id']) + \" | tokens: \" + str(total_tokens))\n",
    "\n",
    "            file_path = r\".\\temp_\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \"_\" + str(row['note_id']) + \".pkl\"\n",
    "            # Save each row output into a file in a temp folder as we go\n",
    "            with open(file_path, 'wb') as file:\n",
    "                pickle.dump(result, file)\n",
    "                \n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing note {row['note_id']}: {e}\")\n",
    "            num_tries = num_tries + 1\n",
    "    \n",
    "    if num_tries >= 3:\n",
    "        print(str(row['note_id'])+ \": failed note\")\n",
    "        result = {\"Status\": 0, \"note_id\": row['note_id']}\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine system context if doing meds\n",
    "system_context = \"\"\"This prompt consists of two sections: \"Instructions\" and \"Input,\" which contains a clinical note for you to parse.\n",
    "\n",
    "Section 1: Instructions\n",
    "\n",
    "Based on the current medication use information from the clinical note given in the Input Section, estimate the value and the certainty degree (CD: 0.00 to 1.00) for the current/ongoing use of the following immunosuppressive medications with allowed values in the bracket:\n",
    "\n",
    "azathioprine (includes azathioprine, imuran, azasan): [yes, no]\n",
    "corticosteroids (only includes oral or IV steroids greater than 5 mg per day): [yes, no]\n",
    "cyclosporine (includes cyclosporine, neoral, sandimmune, gengraf): [yes, no]\n",
    "cyclophosphamide (includes cyclophosphamide, cytoxan): [yes, no]\n",
    "mycophenolate (includes mycophenolate, cellcept, myfortic): [yes, no]\n",
    "myelosuppressive chemotherapy: [yes, no]\n",
    "rituximab (includes rituximab, rituxan, truxima, ruxience, riabni): [yes, no]\n",
    "tacrolimus (includes tacrolimus, prograf, advagraf, astagraf, envarsus, hecoria): [yes, no]\n",
    "\n",
    "Please follow the requirements below:\n",
    "\n",
    "1. For each attribute, create 3 key-value pairs:\n",
    "<attribute_estimation>: <the estimated value based on reading the note>\n",
    "<attribute_CD>: <the certainty degree of your estimation: [0.00, 1.00]>\n",
    "<attribute_evidence>: <the supporting evidence for your estimation>\n",
    "\n",
    "2. Please make sure to make valid inference for attribute estimate based on evidence. If there is no available evidence provided to make estimation, please answer the value as \"unknown\".\n",
    "\n",
    "3. Please make sure to output the whole set of answers together as a single JSON file, and don't output anything beyond the required JSON file.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: read in all notes from a folder and put together with corresponding metadata\n",
    "# Params:\n",
    "#   file: Path, path to metadata file\n",
    "# Produces:\n",
    "#   notes_df: dataframe, has study ID, note ID, note text\n",
    "def load_notes(file):\n",
    "    metadata_df = pd.read_excel(file)\n",
    "    data = []\n",
    "    # Use a raw string (r'') to avoid issues with backslashes in the file path\n",
    "    folder_path = r'PATH TO FOLDER CONTAINING NOTES'\n",
    "    \n",
    "    for _, row in metadata_df.iterrows():\n",
    "        note_id = row['note_id']\n",
    "        script_study_id = row['SCRIPT_study_id']\n",
    "        file_path = os.path.join(folder_path, f'note_{note_id}.txt')\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                data.append({'SCRIPT_study_id': script_study_id, 'note_id': note_id, 'text': text, 'report_type': row['report_type']})\n",
    "        except FileNotFoundError:\n",
    "            print(f'File not found: {file_path}')  # Handle missing files\n",
    "    \n",
    "    notes_df = pd.DataFrame(data)\n",
    "    return notes_df\n",
    "\n",
    "# Read in all the notes\n",
    "hpi_df = load_notes(Path(r'PATH TO NOTES METADATA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each row sequentially\n",
    "results = []\n",
    "for _, row in hpi_df.iterrows():\n",
    "    result = process_row(row)\n",
    "    if result is not None:\n",
    "        results.append(result)\n",
    "\n",
    "# Convert results to DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(r\"./results.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use/modify as needed to get results into the proper format\n",
    "def process_immunosuppression_data(row):\n",
    "    \n",
    "    json_string = row['response']\n",
    "\n",
    "    # Extract the JSON part from the string\n",
    "    json_start = json_string.find('{')\n",
    "    json_end = json_string.rfind('}') + 1\n",
    "    json_data = json_string[json_start:json_end]\n",
    "\n",
    "    # Parse the JSON data\n",
    "    try:\n",
    "        data = json.loads(json_data)\n",
    "    except json.JSONDecodeError:\n",
    "        processed_data = row.to_dict()\n",
    "        df = pd.DataFrame([processed_data])\n",
    "        return df\n",
    "\n",
    "    # Initialize a dictionary to store the processed data\n",
    "    processed_data = row.to_dict()\n",
    "\n",
    "    # Process each condition\n",
    "    for condition, attributes in data.items():\n",
    "        # Check if all required attributes are present\n",
    "        required_attrs = [('attribute_estimation', 'value'), \n",
    "                          ('attribute_CD', 'CD'), \n",
    "                          ('attribute_evidence', 'evidence')]\n",
    "        \n",
    "        for attr, alt_name in required_attrs:\n",
    "            if attr not in attributes and alt_name not in attributes:\n",
    "                processed_data = row.to_dict()\n",
    "                df = pd.DataFrame([processed_data])\n",
    "                return df\n",
    "        \n",
    "        # Add the attributes to the processed data\n",
    "        for attr, alt_name in required_attrs:\n",
    "            key = attr if attr in attributes else alt_name\n",
    "            column_name = f\"{condition}_{attr}\"\n",
    "            processed_data[column_name] = attributes[key]\n",
    "\n",
    "    # Create a DataFrame with a single row\n",
    "    df = pd.DataFrame([processed_data])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish SQL server connection\n",
    "SERVER = 'INSERT SERVER NAME'\n",
    "conn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+SERVER,\n",
    "                      Trusted_Connection='Yes')\n",
    "\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace yes/no/unk with 1/0/0\n",
    "results_df = results_df.replace({'yes': 1, 'no': 0, 'unknown': 0})\n",
    "\n",
    "# Get just the study ID and pred columns so agg works as expected\n",
    "predictions_df = results_df[['SCRIPT_study_id'] + [col for col in results_df.columns if col.endswith('attribute_estimation')]]\n",
    "predictions_df = predictions_df.rename(columns=lambda x: x.replace('_attribute_estimation', '_pred') if '_attribute_estimation' in x else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose:\n",
    "#   generate a confusion matrix for LLM identification of a given immunosuppressive condition and return operating characteristics\n",
    "# Params:\n",
    "#   predictions_df: dataframe, has study ID and all pred columns\n",
    "#   immune_condition: string, how the immunosuppressive condition is named in the prediction column\n",
    "#   gold_label: string, how the immunosuppressive condition is named in the gold standard column\n",
    "#   show_confusion_matrix: boolean, True makes confusion matrix be displayed, False suppresses it\n",
    "#   is_corrections_applied: boolean, True applies manual chart review corrections to gold standard labels, False leaves it as it is\n",
    "#   return_metrics_instead: boolean, True returns metrics_df (see below), False returns final_df (see below)\n",
    "# Produces:\n",
    "#   final_df: dataframe, has study ID, all immune condition predictions, all immune condition gold labels\n",
    "#   metrics_df: dataframe, has all calculated metrics\n",
    "def generate_cm(predictions_df, immune_condition, gold_label, show_confusion_matrix = True, return_metrics_instead = False):\n",
    "\n",
    "    # Load in the IC label data\n",
    "    # Query for SCRIPT 1.0 patients\n",
    "    redcap='''\n",
    "            SET NOCOUNT ON\n",
    "            drop table if exists #imc\n",
    "            select * \n",
    "            into #imc \n",
    "            from (\n",
    "            select distinct pt_study_id,type_immunocomp\n",
    "            from FSM_SCRIPT.fsm_script_redcap_dm.redcap_PROJECTID_demographics\n",
    "            ) x\n",
    "\n",
    "            select pt_study_id,\n",
    "                case when type_immunocomp like '%Acute leukemia%' then 1 end as Leukemia,\n",
    "                case when type_immunocomp like '%Azathioprine%' then 1 end as Azathioprine,\n",
    "                case when type_immunocomp like '%Chronic corticosteroids%' then 1 end as Chronic_corticosteroids,\n",
    "                case when type_immunocomp like '%Cyclosporine%' then 1 end as Cyclosporine,\n",
    "                case when type_immunocomp like '%Cytoxan%' then 1 end as Cytoxan,\n",
    "                case when type_immunocomp like '%HIV%' then 1 end as HIV,\n",
    "                case when type_immunocomp like '%Immunoglobulin deficiency%' then 1 end as Immunoglobulin_deficiency,\n",
    "                case when type_immunocomp like '%Lymphoma%' then 1 end as Lymphoma,\n",
    "                case when type_immunocomp like '%Mycophenolate (MMF)%' then 1 end as Mycophenolate,\n",
    "                case when type_immunocomp like '%Multiple myeloma%' then 1 end as Myeloma,\n",
    "                case when type_immunocomp like '%Myelosuppressive chemotherapy%' then 1 end as Myelosuppressive_chemo,\n",
    "                case when type_immunocomp like '%Rituximab%' then 1 end as Rituximab,\n",
    "                case when type_immunocomp like '%Solid organ transplant%' then 1 end as SOT,\n",
    "                case when type_immunocomp like '%Stem cell transplant%' then 1 end as Stem_cell_transplant,\n",
    "                case when type_immunocomp like '%Tacrolimus%' then 1 end as Tacrolimus\n",
    "            from #imc\n",
    "            '''\n",
    "    \n",
    "    # Same query as above but for SCRIPT 2 patients\n",
    "    redcap2='''\n",
    "            SET NOCOUNT ON\n",
    "            drop table if exists #imc2\n",
    "            select * \n",
    "            into #imc2\n",
    "            from (\n",
    "            select distinct record_id,emr_ic_type\n",
    "            from FSM_SCRIPT.fsm_script_redcap_dm.redcap_PROJECTID_emr_info\n",
    "            ) x\n",
    "\n",
    "            select record_id as pt_study_id,\n",
    "                case when emr_ic_type like '%Acute leukemia%' then 1 end as Leukemia,\n",
    "                case when emr_ic_type like '%Azathioprine%' then 1 end as Azathioprine,\n",
    "                case when emr_ic_type like '%Chronic corticosteroids%' then 1 end as Chronic_corticosteroids,\n",
    "                case when emr_ic_type like '%Cyclosporine%' then 1 end as Cyclosporine,\n",
    "                case when emr_ic_type like '%Cytoxan%' then 1 end as Cytoxan,\n",
    "                case when emr_ic_type like '%HIV%' then 1 end as HIV,\n",
    "                case when emr_ic_type like '%Immunoglobulin deficiency%' then 1 end as Immunoglobulin_deficiency,\n",
    "                case when emr_ic_type like '%Lymphoma%' then 1 end as Lymphoma,\n",
    "                case when emr_ic_type like '%Mycophenolate (MMF)%' then 1 end as Mycophenolate,\n",
    "                case when emr_ic_type like '%Multiple myeloma%' then 1 end as Myeloma,\n",
    "                case when emr_ic_type like '%Myelosuppressive chemotherapy%' then 1 end as Myelosuppressive_chemo,\n",
    "                case when emr_ic_type like '%Rituximab%' then 1 end as Rituximab,\n",
    "                case when emr_ic_type like '%Solid organ transplant%' then 1 end as SOT,\n",
    "                case when emr_ic_type like '%Stem cell transplant%' then 1 end as Stem_cell_transplant,\n",
    "                case when emr_ic_type like '%Tacrolimus%' then 1 end as Tacrolimus\n",
    "            from #imc2\n",
    "            '''\n",
    "    # This dataframe will have study ID and then every immunosuppressive condition listed above and named as above\n",
    "    labels_df_1 = pd.read_sql(redcap, conn)\n",
    "    labels_df_1['pt_study_id'] = labels_df_1['pt_study_id'].astype(int)\n",
    "\n",
    "    # Same dataframe as above but for SCRIPT 2 patients\n",
    "    labels_df_2 = pd.read_sql(redcap2, conn)\n",
    "    labels_df_2['pt_study_id'] = labels_df_2['pt_study_id'].astype(int)\n",
    "\n",
    "    # Put together the labels for SCRIPT 1 and SCRIPT 2\n",
    "    labels_df = pd.concat([labels_df_1, labels_df_2], ignore_index=True)\n",
    "\n",
    "    # Inner join prediction_df to labels_df to put predictions and labels together (TODO: in ICD version it's right join, make sure this doesn't matter)\n",
    "    final_df = pd.merge(left = predictions_df, right = labels_df, left_on = 'SCRIPT_study_id', right_on = 'pt_study_id', how = 'inner')\n",
    "    final_df = final_df.fillna(0)\n",
    "\n",
    "    # Make sure the types match for confusion matrix input\n",
    "    final_df[gold_label] = final_df[gold_label].astype(int)\n",
    "    final_df[immune_condition] = final_df[immune_condition].astype(int)\n",
    "\n",
    "    # Create and display the confusion matrix\n",
    "    cm = confusion_matrix(final_df[gold_label], final_df[immune_condition], labels = [0, 1])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    sensitivity = recall_score(final_df[gold_label], final_df[immune_condition])\n",
    "    specificity = tn / (tn + fp)\n",
    "    ppv = precision_score(final_df[gold_label], final_df[immune_condition])\n",
    "    npv = tn / (tn + fn)\n",
    "    accuracy = accuracy_score(final_df[gold_label], final_df[immune_condition])\n",
    "    f1 = f1_score(final_df[gold_label], final_df[immune_condition])\n",
    "\n",
    "    # Draw confusion matrix\n",
    "    cm1 = cm[::-1, ::-1]\n",
    "\n",
    "    # Only display the CM if asked for\n",
    "    if show_confusion_matrix:\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        sns.heatmap(cm1, annot=True, fmt='d', cmap='Purples',\n",
    "                xticklabels=['LLM 1', 'LLM 0'],\n",
    "                yticklabels=['REDCap 1', 'REDCap 0'])\n",
    "        plt.title(f'Confusion Matrix - {immune_condition}')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.show()\n",
    "        #plt.savefig(f'confusion_matrix_{comparison_name}.png')\n",
    "        #plt.close()\n",
    "\n",
    "    # Create DataFrame with metrics\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Comparison': [immune_condition],\n",
    "        'True Negative': [tn],\n",
    "        'False Positive': [fp],\n",
    "        'False Negative': [fn],\n",
    "        'True Positive': [tp],\n",
    "        'Sensitivity': [sensitivity],\n",
    "        'Specificity': [specificity],\n",
    "        'PPV': [ppv],\n",
    "        'NPV': [npv],\n",
    "        'Accuracy': [accuracy],\n",
    "        'F1 Score': [f1]\n",
    "    })\n",
    "\n",
    "    # And only print the metrics if asked for\n",
    "    if show_confusion_matrix:\n",
    "        print(metrics_df)\n",
    "\n",
    "    if return_metrics_instead:\n",
    "        return metrics_df\n",
    "    else:\n",
    "        return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For doing conditions\n",
    "\n",
    "pred_list = ['solid_organ_transplant_pred', 'stem_cell_transplant_pred', 'HIV_pred', 'acute_leukemia_pred', 'lymphoma_pred', 'multiple_myeloma_pred', 'immunoglobulin_deficiency_pred']\n",
    "gold_list = ['SOT', 'Stem_cell_transplant', 'HIV', 'Leukemia', 'Lymphoma', 'Myeloma', 'Immunoglobulin_deficiency']\n",
    "\n",
    "# Run generate_cm() for all the conditions with corrections_applied\n",
    "for pred, gold in zip(pred_list, gold_list):\n",
    "    generate_cm(predictions_df, pred, gold, is_corrections_applied = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For doing meds\n",
    "\n",
    "pred_list = ['azathioprine_pred', 'corticosteroids_pred', 'cyclosporine_pred', 'cyclophosphamide_pred', 'mycophenolate_pred', 'myelosuppressive_chemotherapy_pred', 'rituximab_pred', 'tacrolimus_pred']\n",
    "gold_list = ['Azathioprine', 'Chronic_corticosteroids', 'Cyclosporine', 'Cytoxan', 'Mycophenolate', 'Myelosuppressive_chemo', 'Rituximab', 'Tacrolimus']\n",
    "\n",
    "# Run generate_cm() for all the conditions with corrections_applied\n",
    "for pred, gold in zip(pred_list, gold_list):\n",
    "    generate_cm(predictions_df, pred, gold, is_corrections_applied = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile corrected metrics\n",
    "\n",
    "# Initialize an empty list to store individual metric DataFrames\n",
    "metrics_list = []\n",
    "\n",
    "for pred, gold in zip(pred_list, gold_list):\n",
    "    # Generate metrics for each prediction\n",
    "    metrics_df = generate_cm(predictions_df, pred, gold, show_confusion_matrix=False, is_corrections_applied=True, return_metrics_instead=True)\n",
    "    # Append the metrics DataFrame to the list\n",
    "    metrics_list.append(metrics_df)\n",
    "\n",
    "# Concatenate all individual metric DataFrames into a single DataFrame\n",
    "final_metrics_df = pd.concat(metrics_list, ignore_index=True)\n",
    "\n",
    "# Save metrics as a CSV in this directory for subsequent figure generation\n",
    "final_metrics_df.to_csv(Path(f'metrics.csv'), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ID_IC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
